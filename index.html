<html>
<head>
<TITLE> Michael Bukatin - Dataflow Matrix Machines (since 2017) </TITLE>
</head>

<body>

<H1 align=center>Dataflow Matrix Machines (since 2017)</H1>

<HR>

A class of expressive self-modifiable neural machines
<UL>
<LI> which can fluently modify their own weights, connectivity patterns, and size;
<LI> and can serve as a programming platform, resulting in a programming formalism
     with continuously deformable programs.
</UL>

<HR>

<H3>2015-2017</H3>

<P>Timeline and details: <A HREF="partial_inconsistency.html">Partial Inconsistency and
Vector Semantics of Programming Languages</A>.</P>


<P>Reference paper: Michael Bukatin and Jon Anthony. <A HREF="https://arxiv.org/abs/1712.07447">Dataflow 
Matrix Machines and V-values: a Bridge between Programs and Neural Nets</A>. 
In "K + K = 120" Festschrift, 2017.
</P>

<HR>

<H3>2018</H3>

<P> The slides for my talk at 
<A HREF="https://researcher.watson.ibm.com/researcher/view_group_subpage.php?id=10084">IBM AI Systems Day 2018</A>, Cambridge, MA, October 3, 2018:
<A HREF="https://web.archive.org/web/20220305051310/https://researcher.watson.ibm.com/researcher/files/us-lmandel/aisys18-bukatin.pdf">Dataflow
Matrix Machines and V-values: a Bridge between Programs and Neural Nets</A> (joint
work with Jon Anthony).</P>

<P>(The file with slides <A HREF="https://researcher.watson.ibm.com/researcher/files/us-lmandel/aisys18-bukatin.pdf">https://researcher.watson.ibm.com/researcher/files/us-lmandel/aisys18-bukatin.pdf</A> has moved. This file is referenced in a number of PDFs on this page. Use <A HREF="https://web.archive.org/web/20220305051310/https://researcher.watson.ibm.com/researcher/files/us-lmandel/aisys18-bukatin.pdf">https://web.archive.org/web/20220305051310/https://researcher.watson.ibm.com/researcher/files/us-lmandel/aisys18-bukatin.pdf</A> also known as <A HREF="https://tinyurl.com/ibm-aisys18-bukatin">https://tinyurl.com/ibm-aisys18-bukatin</A>)</P>

<P>DMM  technical  report  11-2018.
<A HREF="dmm-notes-2018.pdf">Dataflow  matrix  machines:   recent  experiments  and  notes  for  next  steps</A>. Preprint, November 2018.
</P>

<HR>

<H3>2019</H3>

<P><A HREF="https://github.com/anhinga/synapses/blob/master/regularization.md">Regularization in intrinsically sparse networks</A>. An
experimental study, February-March 2019. 

<P>A <b>white paper</b> on dataflow matrix machines: <A HREF="dmm-white-paper-2019.pdf">dmm-white-paper-2019.pdf</A> (2 pages, November 2019).</P>

<P><A HREF="https://github.com/anhinga/2019-design-notes/tree/master/research-notes">2019 research notes on dataflow matrix machines.</A> </P>

<P>An interdisciplinary and collaborative research agenda related to dataflow matrix machines: <A HREF="https://github.com/anhinga/2019-design-notes/tree/master/research-agenda">Version 1</A> (4 pages, December 2019).</P>

<HR>

<H3>2020</H3>

<P>Using streams of probabilistic samples in neural machines: <A HREF="dmm-probabilistic-samples.pdf">dmm-probabilistic-samples.pdf</A> (January 2020).</P>

<P>Synergy between AI-generating algorithms and dataflow matrix machines: <A HREF="synergy-dmm-ai-ga.pdf">synergy-dmm-ai-ga.pdf</A> (March 2020).</P>

<P>An interdisciplinary and collaborative research agenda related to dataflow matrix machines: <A HREF="dmm-collaborative-research-agenda.pdf">dmm-collaborative-research-agenda.pdf</A> (7 pages, August 2020; updated April 2022).</P>

<P><A HREF="https://github.com/anhinga/2020-notes/tree/master/CCC-2020">Higher-order neuromorphic computations with linear streams</A>. Extended abstract and slides for my CCC 2020 talk (September 3, 2020).</P>

<HR>

<H3>2021</H3>

Work continues on
<UL>
<LI> interplay between DMMs and Transformers
<LI> interplay between DMMs and Differentiable Programming
</UL>

<P>Towards practical use of dataflow matrix machines: <A HREF="towards-practical-dmms.pdf">towards-practical-dmms.pdf</A> (March 2021; updated April 2022).</P>

<P><A HREF="https://github.com/anhinga/JuliaCon2021-poster">Multiplying monochrome images as matrices: A*B and softmax.</A>
A virtual poster presented at JuliaCon 2021 (July 2021)</P>

<HR>

<H3>2022</H3>

Applications of differentiable programming to DMMs and V-values:
<UL>
<LI>Gradients with respect to values stored inside trees work (Julia, JAX, April 2022)
<LI>Gradients through variadic DMMs with dictionary flows work (Julia, May 2022)
<LI>DMM training by gradient method works for some classes of variadic DMMs with dictionary flows (Julia, June 2022)
<LI>A primer on performing <b>program synthesis and DMM synthesis via a neural architecture search algorithm based on differentiable programming</b> works (Julia, June 2022)
</UL>

<HR>
<HR>

<ADDRESS>
Dataflow matrix machines <A HREF="https://anhinga.github.io/">GitHub Pages site</A> 
</ADDRESS>

</body>

</html>
